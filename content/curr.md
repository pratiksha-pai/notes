mainly from https://twitter.com/var_epsilon

predictions, patterns, and actions: mlstory.org
karpathy: https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ
transformers from scratch: https://e2eml.school/transformers.html
gpt in 60 lines: https://jaykmody.com/blog/gpt-from-scratch/
cuda matmul kernel: https://siboehm.com/articles/22/CUDA-MMM
autonomous ai agents: https://lilianweng.github.io/posts/2023-06-23-agent/
ml interviews book: https://huyenchip.com/ml-interviews-book/
novice’s guide to LLM training: https://rentry.co/llm-training
transformer math: https://blog.eleuther.ai/transformer-math/#total-inference-memory
nonint: https://nonint.com/
llama v2
a16z infra llama chat: https://replicate.com/a16z-infra/llama-2-13b-chat
hf model: https://huggingface.co/TheBloke/Llama-2-13B-GGML
finetuning: https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32
flashattention explainer: https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad
kernel cookbook: https://www.cs.toronto.edu/~duvenaud/cookbook/
horace he fast DL: https://horace.io/brrr_intro.html
long term memory: http://augmentingcognition.com/ltm.html
kalman filters: https://praveshkoirala.com/2023/06/13/a-non-mathematical-introduction-to-kalman-filters-for-programmers/
google’s grokking explainer: https://pair.withgoogle.com/explorables/grokking/
how is llama possible: https://finbarr.ca/how-is-llama-cpp-possible/
SAIL courses: https://ai.stanford.edu/courses/
stanford smallville, interactive simulacra: https://github.com/joonspk-research/generative_agents
ai cannon: https://a16z.com/2023/05/25/ai-canon/
ilya https://youtu.be/AKMuA_TVz3A
geohot https://github.com/geohot/fromthetransistor/